obecnie testowane:
22.17



12- rerank na 10
dev 25.18
test-A 54.61



13 - zwiększenie z 10 do 30 
dev  28.81
test-A 57.2


15 zwiekszanie rerank do 50
dev  29.76
test-A 57.45


16 zwiekszanie rerank do 70
dev 30.63
test-A 57.65

17 rerank 100
model_name='cross-encoder/ms-marco-MiniLM-L-6-v2'
dev 31.48
test  57.71


19 
zmiana z 
model_name='cross-encoder/ms-marco-MiniLM-L-6-v2'
na
model_name='cross-encoder/ms-marco-MiniLM-L-12-v2'
dev 31.61
test-A 58.29



20
zmiana modelu na multilingual i użycie języka polskiego
model_name='cross-encoder/mmarco-mMiniLMv2-L12-H384-v1'
dev 36.25
test-A 66.31

21
cross-encoder/mmarco-mdeberta-v3-base-5negs-v1
dev 38.40
test-A 68.39


22
jak poprzednio tylko reranking z 200, a nie 100
dev 40.05
test-A 69.01



dev 33.52
100, ale 
model_name = 'amberoad/bert-multilingual-passage-reranking-msmarco'
nie wrzucałem na strone



23
cross-encoder/mmarco-mdeberta-v3-base-5negs-v1 
rerank z 300
dev 41.25
test-A 69.09


24 
rerank z 500
dev 42.70
test-A 69.31



-----
model_name='cross-encoder/mmarco-mMiniLMv2-L12-H384-v1'
rarank 500 oraz truncation='only_second' 
dev 38.65


25
ensemble z 
model_name1='cross-encoder/mmarco-mMiniLMv2-L12-H384-v1'
model_name2='cross-encoder/mmarco-mdeberta-v3-base-5negs-v1'
dla legal i allegro trzeba zredukować BS do 50
ale nie ma żadnego ważenia, a min/max są chyba rózne, można użyć minmaxscaler
z 500
dev 43.0 
test-A 69.62


26 jw, tylko skalibrowane wagi zeby było po równo
dev 69.4



28
zwiekszenie limitu na 2000 dokumentów do rerankowania,
ensemble z 2:
model_name1='cross-encoder/mmarco-mMiniLMv2-L12-H384-v1'
model_name2='cross-encoder/mmarco-mdeberta-v3-base-5negs-v1'
dev-0 45.08
test-A 70.30



tak jak 28 tylko sama deberta:
dev 44.84


trening na train_dataset_for_rerank_50_negs_1000.pickle
oraz ewaluacja na dev-0_dataset_for_rerank_5_negs_30.pickle
branie 500 minilm:
bez finetuningu
dev 36.64

dostrajanie minilm best epoch
dev 42.66

dostrajanie minilm last epoch
dev 42.41


finetunowany cross-encoder-mmarco-mdeberta-v3-base-5negs-v1-2022-12-12_08-57-01
dev 45.49 rerank na 2000



model_name1='/mnt/gpu_data1/kubapok/poleval2022/solutions/fastbm25-train-reranker/output/cross-encoder-mmarco-mdeberta-v3-base-5negs-v1-2022-12-12_08-57-01'
z 2000
dev 48.84


29
model_name1='/mnt/gpu_data1/kubapok/poleval2022/solutions/fastbm25-train-reranker/output/cross-encoder-mmarco-mdeberta-v3-base-5negs-v1-2022-12-12_08-57-01'
dotrenowywanie
z 3000
dev 48.21
test-A - 29.06

30
zwiekszenie limitu na 3000 dokumentów do rerankowania,
ensemble z 2:
model_name1='cross-encoder/mmarco-mMiniLMv2-L12-H384-v1'
model_name2='cross-encoder/mmarco-mdeberta-v3-base-5negs-v1'
dev 45.53
test 70.26


31- ensembl z 29 -i 30 wiki z 29, a legal, allegro z 30
test-A 70.87


32
legal i allegro tak jak w 30
wiki ensemble z 
cross-encoder/mmarco-mdeberta-v3-base-5negs-v1 oraz '/mnt/gpu_data1/kubapok/poleval2022/solutions/fastbm25-train-reranker/output/cross-encoder-mmarco-mdeberta-v3-base-5negs-v1-2022-12-12_08-57-01'
allegro i legal ensemble z 
model_name1='cross-encoder/mmarco-mMiniLMv2-L12-H384-v1'
model_name2='cross-encoder/mmarco-mdeberta-v3-base-5negs-v1'
dev-0 51.60
test-A 72.23


33 -minimarco finetunowany do wiki
